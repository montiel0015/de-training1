{
   "paragraphs": [
      {
         "text": "%md\n# Iterations and matrix multiplication\n\nWelcome to the notebook with the asasignment for the second session. You\u2019re well on your way to obtain the Wizeline Certification for Big Data Engineering with Spark!\n\nIf you have any feedback about our courses, email us at academy@wizeline.com or use the Academy Slack channel.",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "apps": [],
         "jobName": "paragraph_1534456393716_1675754884",
         "id": "20180815-184319_767984790",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "focus": true,
         "$$hashKey": "object:232"
      },
      {
         "text": "%md\n## Matrix Multiplication Assignment\n\nIn this exercise, you will be implementing a Matrix multiplication on a larger dataset than the one you\u2019ve used so far. \n\nYou will be challenged to work with two Datasets, transform them into `CoordinateMatrix` objects, and ultimately convert each to a `BlockMatrix`.\n",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<h2>Matrix Multiplication Assignment</h2>\n<p>In this exercise, you will be implementing a Matrix multiplication on a larger dataset than the one you\u2019ve used so far. </p>\n<p>You will be challenged to work with two Datasets, transform them into <code>CoordinateMatrix</code> objects, and ultimately convert each to a <code>BlockMatrix</code>.</p>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393717_1675370135",
         "id": "20180816-044039_285868597",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:233"
      },
      {
         "text": "%md\n\n```\nFile Format:\nEach line in the text file is a row in the matrix. They are comma-separated values. The first value is the row index starting at one, the remaining are the Matrix values. \n```\n\nFirst, import the libraries to use during the session with the following command:\n\n\n",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<pre><code>File Format:\nEach line in the text file is a row in the matrix. They are comma-separated values. The first value is the row index starting at one, the remaining are the Matrix values. \n</code></pre>\n<p>First, import the libraries to use during the session with the following command:</p>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393722_1674985386",
         "id": "20180816-050807_2103207409",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:234"
      },
      {
         "text": "import org.apache.spark.sql.Dataset\nimport org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": true,
            "editorSetting": {
               "language": "scala"
            },
            "colWidth": 12,
            "editorMode": "ace/mode/scala",
            "editorHide": false,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "TEXT",
                  "data": "import org.apache.spark.sql.Dataset\nimport org.apache.spark.mllib.linalg.distributed.{BlockMatrix, CoordinateMatrix, MatrixEntry}\n"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393722_1674985386",
         "id": "20180816-051038_545285622",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:235"
      },
      {
         "text": "%md\nNext use `spark.read.textFile` to read the two matrices (matrix1.txt and matrix2.txt) on the bucket URI: \n`gs://de-training-input/matrices/matrix*.txt`.\n\nBecause there are two matrices, make a read call for each and store them in separate variables m1s and m2s. \n\nDon\u2019t forget to map a split by \u201c,\u201d while reading the files.\n",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<p>Next use <code>spark.read.textFile</code> to read the two matrices (matrix1.txt and matrix2.txt) on the bucket URI:<br/><code>gs://de-training-input/matrices/matrix*.txt</code>.</p>\n<p>Because there are two matrices, make a read call for each and store them in separate variables m1s and m2s. </p>\n<p>Don\u2019t forget to map a split by \u201c,\u201d while reading the files.</p>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393723_1674600637",
         "id": "20180816-051039_1898536718",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:236"
      },
      {
         "text": "",
         "dateUpdated": "2018-08-16T22:06:38+0000",
         "config": {
            "colWidth": 12,
            "editorMode": "ace/mode/scala",
            "results": {},
            "enabled": true,
            "editorSetting": {
               "language": "scala"
            }
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "TEXT",
                  "data": "m1s: org.apache.spark.sql.Dataset[Array[String]] = [value: array<string>]\nm2s: org.apache.spark.sql.Dataset[Array[String]] = [value: array<string>]\n"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393723_1674600637",
         "id": "20180816-070420_1766932377",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:237"
      },
      {
         "text": "%md\nNow that we have loaded the data into Spark, we need to transform them into the `MatrixEntry` class that was imported earlier. To do so use a `flatMap` to create the required objects. \n\nYour mapper will need to iterate over each item in the row to correctly yield the parameters required by `MatrixEntry`. If you use `.view.zipWithIndex` on each line, you get an iterator that will contain (`value, list_index`).\n\n\nHINT: How do you remove the first value from each line from the `.zipWithIndex` method call? Does `head` ring a bell?\n",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<p>Now that we have loaded the data into Spark, we need to transform them into the <code>MatrixEntry</code> class that was imported earlier. To do so use a <code>flatMap</code> to create the required objects. </p>\n<p>Your mapper will need to iterate over each item in the row to correctly yield the parameters required by <code>MatrixEntry</code>. If you use <code>.view.zipWithIndex</code> on each line, you get an iterator that will contain (<code>value, list_index</code>).</p>\n<p>HINT: How do you remove the first value from each line from the <code>.zipWithIndex</code> method call? Does <code>head</code> ring a bell?</p>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393723_1674600637",
         "id": "20180816-203508_664801339",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:238"
      },
      {
         "text": "",
         "dateUpdated": "2018-08-16T22:06:44+0000",
         "config": {
            "colWidth": 12,
            "editorMode": "ace/mode/scala",
            "results": {},
            "enabled": true,
            "editorSetting": {
               "language": "scala"
            }
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "TEXT",
                  "data": "mEntries1: org.apache.spark.sql.Dataset[org.apache.spark.mllib.linalg.distributed.MatrixEntry] = [i: bigint, j: bigint ... 1 more field]\nmEntries2: org.apache.spark.sql.Dataset[org.apache.spark.mllib.linalg.distributed.MatrixEntry] = [i: bigint, j: bigint ... 1 more field]\n"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393723_1674600637",
         "id": "20180816-070426_2002522120",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:239"
      },
      {
         "text": "%md\nNow that we have converted all of our lines into `MatrixEntry` objects we need to put them all inside a `BlockMatrix`. Define a function called `create_blockMatrix` that receives one of the previous objects and uses a `CoordinateMatrix` to output a `BlockMatrix`.\n",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<p>Now that we have converted all of our lines into <code>MatrixEntry</code> objects we need to put them all inside a <code>BlockMatrix</code>. Define a function called <code>create_blockMatrix</code> that receives one of the previous objects and uses a <code>CoordinateMatrix</code> to output a <code>BlockMatrix</code>.</p>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393723_1674600637",
         "id": "20180816-052746_2109723812",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:240"
      },
      {
         "text": "",
         "dateUpdated": "2018-08-16T22:06:47+0000",
         "config": {
            "colWidth": 12,
            "editorMode": "ace/mode/scala",
            "results": {},
            "enabled": true,
            "editorSetting": {
               "language": "scala"
            }
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "TEXT",
                  "data": "create_blockMatrix: (MatrixEntries: org.apache.spark.sql.Dataset[org.apache.spark.mllib.linalg.distributed.MatrixEntry])org.apache.spark.mllib.linalg.distributed.BlockMatrix\n"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393724_1672676892",
         "id": "20180816-051640_149771352",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:241"
      },
      {
         "text": "%md\nNow, we convert each `Dataset` into a `BlockMatrix`:",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<p>Now, we convert each <code>Dataset</code> into a <code>BlockMatrix</code>:</p>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393724_1672676892",
         "id": "20180816-051434_624669525",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:242"
      },
      {
         "text": "val blockM1 = create_blockMatrix(mEntries1)\nval blockM2 = create_blockMatrix(mEntries2)",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "colWidth": 12,
            "editorMode": "ace/mode/scala",
            "results": {},
            "enabled": true,
            "editorSetting": {
               "language": "scala",
               "editOnDblClick": false
            }
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "TEXT",
                  "data": "blockM1: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@711b6658\nblockM2: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@52f64d84\n"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393724_1672676892",
         "id": "20180816-053544_199438445",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:243"
      },
      {
         "text": "%md\nFinally, to perform the optimized multiplication use the multiply method of the `BlockMatrix`. As an exercise multiply `blockM1` against `blockM2`. Store the result in a variable.\n\n",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "editorSetting": {
               "language": "scala"
            },
            "colWidth": 12,
            "editorMode": "ace/mode/scala",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<p>Finally, to perform the optimized multiplication use the multiply method of the <code>BlockMatrix</code>. As an exercise multiply <code>blockM1</code> against <code>blockM2</code>. Store the result in a variable.</p>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393724_1672676892",
         "id": "20180816-053542_76914317",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:244"
      },
      {
         "text": "",
         "dateUpdated": "2018-08-16T22:06:54+0000",
         "config": {
            "colWidth": 12,
            "editorMode": "ace/mode/scala",
            "results": {},
            "enabled": true,
            "editorSetting": {
               "language": "scala"
            }
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "TEXT",
                  "data": "result: org.apache.spark.mllib.linalg.distributed.BlockMatrix = org.apache.spark.mllib.linalg.distributed.BlockMatrix@616e372d\n"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393724_1672676892",
         "id": "20180816-054044_1121238451",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:245"
      },
      {
         "text": "%md\nOnce the computation is over, save the results to your output bucket. To do so, you will need to transform the `result` object into a `CoordinateMatrix`. Then point to its `entries` attribute, using the `saveAsTextFile` method. Use the following GCS URI template:\nMake sure to replace `<user-name>` with your personal username ID.\n",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<p>Once the computation is over, save the results to your output bucket. To do so, you will need to transform the <code>result</code> object into a <code>CoordinateMatrix</code>. Then point to its <code>entries</code> attribute, using the <code>saveAsTextFile</code> method. Use the following GCS URI template:<br/>Make sure to replace <code>&lt;user-name&gt;</code> with your personal username ID.</p>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393725_1672292143",
         "id": "20180816-051040_524958004",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:246"
      },
      {
         "text": "",
         "dateUpdated": "2018-08-16T22:07:06+0000",
         "config": {
            "tableHide": true,
            "editorSetting": {
               "language": "scala"
            },
            "colWidth": 12,
            "editorMode": "ace/mode/scala",
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "ERROR",
            "msg": [
               {
                  "type": "TEXT",
                  "data": "java.lang.IllegalArgumentException: Invalid bucket name (de-training-output-<user-name>) or object name ()\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:99)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem.configureBuckets(GoogleHadoopFileSystem.java:75)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.configure(GoogleHadoopFileSystemBase.java:2011)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1102)\n  at com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystemBase.initialize(GoogleHadoopFileSystemBase.java:1065)\n  at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2812)\n  at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:100)\n  at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2849)\n  at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2831)\n  at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:389)\n  at org.apache.hadoop.fs.Path.getFileSystem(Path.java:356)\n  at org.apache.spark.internal.io.SparkHadoopWriterUtils$.createPathFromString(SparkHadoopWriterUtils.scala:55)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply$mcV$sp(PairRDDFunctions.scala:1069)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$4.apply(PairRDDFunctions.scala:1035)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1035)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply$mcV$sp(PairRDDFunctions.scala:961)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n  at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopFile$1.apply(PairRDDFunctions.scala:961)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:960)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply$mcV$sp(RDD.scala:1489)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n  at org.apache.spark.rdd.RDD$$anonfun$saveAsTextFile$1.apply(RDD.scala:1468)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n  at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\n  at org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1468)\n  ... 46 elided\nCaused by: java.net.URISyntaxException: Illegal character in authority at index 5: gs://de-training-output-<user-name>/\n  at java.net.URI$Parser.fail(URI.java:2848)\n  at java.net.URI$Parser.parseAuthority(URI.java:3186)\n  at java.net.URI$Parser.parseHierarchical(URI.java:3097)\n  at java.net.URI$Parser.parse(URI.java:3053)\n  at java.net.URI.<init>(URI.java:588)\n  at com.google.cloud.hadoop.gcsio.LegacyPathCodec.getPath(LegacyPathCodec.java:93)\n  ... 78 more\n"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393725_1672292143",
         "id": "20180816-055550_1811176051",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:247"
      },
      {
         "text": "%md\n**Note:** we are converting the `BlockMatrix` back to `CoordinateMatrix` for writing purposes.",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<p><strong>Note:</strong> we are converting the <code>BlockMatrix</code> back to <code>CoordinateMatrix</code> for writing purposes.</p>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393725_1672292143",
         "id": "20180816-055549_596649276",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:248"
      },
      {
         "text": "%md\n## Ensure you convert this notebook into a JAR to submit directly to your cluster for the output to be valid",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "tableHide": false,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            },
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "editorHide": true,
            "results": {},
            "enabled": true
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "results": {
            "code": "SUCCESS",
            "msg": [
               {
                  "type": "HTML",
                  "data": "<div class=\"markdown-body\">\n<h2>Ensure you convert this notebook into a JAR to submit directly to your cluster for the output to be valid</h2>\n</div>"
               }
            ]
         },
         "apps": [],
         "jobName": "paragraph_1534456393725_1672292143",
         "id": "20180816-061612_1285669371",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:249"
      },
      {
         "text": "%md\n",
         "dateUpdated": "2018-08-16T21:53:13+0000",
         "config": {
            "colWidth": 12,
            "editorMode": "ace/mode/markdown",
            "results": {},
            "enabled": true,
            "editorSetting": {
               "language": "markdown",
               "editOnDblClick": true
            }
         },
         "settings": {
            "params": {},
            "forms": {}
         },
         "apps": [],
         "jobName": "paragraph_1534456393725_1672292143",
         "id": "20180816-204005_1297364994",
         "dateCreated": "2018-08-16T21:53:13+0000",
         "status": "READY",
         "errorMessage": "",
         "progressUpdateIntervalMs": 500,
         "$$hashKey": "object:250"
      }
   ],
   "name": "Matrix-Multiplication-Scala",
   "id": "2DPYFWWH8",
   "angularObjects": {
      "2DPV5GYZV:shared_process": [],
      "2DMXVEND8:shared_process": [],
      "2DP2JZ1T7:shared_process": [],
      "2DMQCTGAC:shared_process": [],
      "2DPT2WPZN:shared_process": [],
      "2DMAE54WY:shared_process": [],
      "2DMJQGHDP:shared_process": [],
      "2DQXFTHN6:shared_process": [],
      "2DNHU9QDY:shared_process": [],
      "2DPC2RS1N:shared_process": [],
      "2DMMWEW7N:shared_process": [],
      "2DNSBFHYA:shared_process": [],
      "2DPPW98XB:shared_process": [],
      "2DP8U9D6K:shared_process": [],
      "2DN67JH1V:shared_process": [],
      "2DNHYRTT8:shared_process": [],
      "2DMJY7M94:shared_process": [],
      "2DQQ5WYPM:shared_process": []
   },
   "config": {
      "looknfeel": "default",
      "personalizedMode": "false"
   },
   "info": {}
}
